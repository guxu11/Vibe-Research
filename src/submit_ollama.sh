#!/bin/bash
#SBATCH --job-name=ollama_summary
#SBATCH --output=ollama_summary.log
#SBATCH --gres=gpu:1
#SBATCH --time=04:00:00
#SBATCH --partition=gpucluster
#SBATCH --cpus-per-task=4

module load cuda/11.8  # åŠ è½½ GPU ä¾èµ–

# å¼ºåˆ¶åˆ‡æ¢åˆ°æ­£ç¡®çš„å·¥ä½œç›®å½•
cd ~/project/Vibe-Research/src || exit 1

# æ¿€æ´» Python è™šæ‹ŸçŽ¯å¢ƒ
source ~/project/Vibe-Research/.venv/bin/activate
export PATH=$HOME/.local/bin:$PATH

# ç¡®ä¿ Python è¿è¡Œæ­£å¸¸
echo "ðŸ” Python path: $(which python3)" | tee -a make_summaries.log
python3 --version | tee -a make_summaries.log

# å¯åŠ¨ Ollama æœåŠ¡å™¨
nohup ollama serve > ollama_server.log 2>&1 & disown
sleep 5  

# ç¡®ä¿ Ollama æœåŠ¡å™¨å¯åŠ¨æˆåŠŸ
RETRIES=15
while ! curl -s http://localhost:11434/api/tags > /dev/null; do
    echo "ðŸ”„ Ollama still starting..." | tee -a make_summaries.log
    sleep 4
    ((RETRIES--))
    if [ $RETRIES -le 0 ]; then
        echo "âŒ Ollama failed to start!" | tee -a make_summaries.log
        exit 1
    fi
done

echo "âœ… Ollama is ready!" | tee -a make_summaries.log

# è¿è¡Œ Python è„šæœ¬
echo "ðŸš€ Running Python script..." | tee -a make_summaries.log
python3 -u make_summaries.py 2>&1 | tee -a make_summaries.log

echo "âœ… Python script execution finished." | tee -a make_summaries.log

