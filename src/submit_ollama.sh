#!/bin/bash
#SBATCH --job-name=ollama_summary
#SBATCH --output=ollama_summary_%A_%a.log  # æ¯ä¸ªä»»åŠ¡æœ‰å•ç‹¬çš„æ—¥å¿—
#SBATCH --gres=gpu:1
#SBATCH --time=08:00:00
#SBATCH --partition=gpucluster
#SBATCH --cpus-per-task=4
#SBATCH --array=0-3  # 4 ä¸ªä»»åŠ¡ï¼Œå¯¹åº” 4 å— GPU

cd ~/project/Vibe-Research/src || exit 1

# æ¿€æ´» Python è™šæ‹Ÿç¯å¢ƒ
source ~/project/Vibe-Research/.venv/bin/activate
export PATH=$HOME/.local/bin:$PATH

# **ç»‘å®š GPU**
export CUDA_VISIBLE_DEVICES=$SLURM_ARRAY_TASK_ID
export OLLAMA_CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES

# **åˆ†é… Ollama ç«¯å£**
PORT=$((11434 + SLURM_ARRAY_TASK_ID))
export OLLAMA_API_HOST="http://127.0.0.1:$PORT"

# **å¯åŠ¨ Ollama æœåŠ¡å™¨**
nohup ollama serve > ollama_server_${SLURM_ARRAY_TASK_ID}.log 2>&1 & disown
sleep 5

# **ç­‰å¾… Ollama å¯åŠ¨**
RETRIES=15
while ! curl -s ${OLLAMA_API_HOST}/api/tags > /dev/null; do
    echo "ğŸ”„ Ollama still starting on ${OLLAMA_API_HOST}..."
    sleep 4
    ((RETRIES--))
    if [ $RETRIES -le 0 ]; then
        echo "âŒ Ollama failed to start!"
        exit 1
    fi
done

echo "âœ… Ollama is ready on ${OLLAMA_API_HOST}!"

# **è¿è¡Œ Python è„šæœ¬**
echo "ğŸš€ Running Python script with param: ${SLURM_ARRAY_TASK_ID} on GPU ${CUDA_VISIBLE_DEVICES}"
python3 -u make_summaries.py ${SLURM_ARRAY_TASK_ID} ${OLLAMA_API_HOST}

echo "âœ… Python script execution finished."
